### 1 动机

#### 1.1 前情回顾

课程至今已经介绍的内容包括

- 密度估计模型：自回归、流模型、变分推断
- 隐含似然模型：生成对抗网络、能量模型
- 生成模型的应用

#### 1.2 今日任务

- 如何从原始的无标签数据中学习丰富而有用的特征，对一些下游任务有用？
- 有哪些不同的前置（代理）任务可以用来从无标签数据中学习表征？
- 如何通过良好的预训练网络来提高数据效率和下游任务的性能？

#### 1.3 自监督学习

- 常与无监督学习互换使用(使用数据产生的标签)；
- 通过上游人物创造出自身的标签：例如，遮挡图片的一部分作为输入，让nn预测被遮挡的部分；
- 为什么需要自监督学习：
  - 1) 创造标签成本较高 
  - 2) 可以充分利用互联网上的无标签数据 
  - 3) 研究认知：动物与小宝宝是如何学习的
- 自监督学习目标：
  - 1) 在无监督情况下，学习到与监督式学习一样好的特征；
  - 2) 部署完成后，在下游任务中不需要太多标签；
  - 3) 潜在的泛化性更好。
- 做法：假设输入的一部分未知，然后预测它。
- 认知原则：
  - 从混乱/部分数据中重构：去噪自编码器、图像修复、上色、Split-Brain 自编码器；
  - 视觉常见任务：图片部分内容的相关性、拼图、旋转；
  - 对比学习：word2vec、对比预测编码(CPC)、实例辨析、其他SOTA模型。

### 2 自监督学习举例

**因初次接触此范畴的知识内容，以下仅对各种模型做出简要介绍。**

#### 2.1 去噪自编码器

- 实现方法：在数据中添加噪声然后使用nn去除

  ![](fig\fig7\fig1.png)

- 噪声种类：

  - 1) 各向同性高斯噪声$\hat{\mathbf{x}}|\mathbf{x} \sim \mathcal{N}(\mathbf{x},\sigma^2\mathbf{I})$；
  - 2) 掩膜噪声：一部分像素置0；
  - 3) 椒盐噪声：一部分像素置0或1。

- 损失函数：二次型或交叉熵

  $L_{2, \alpha}(\mathbf{x}, \mathbf{z})=\alpha\left(\sum_{j \in \mathcal{J}(\tilde{\mathbf{x}})}\left(\mathbf{x}_{j}-\mathbf{z}_{j}\right)^{2}\right)+\beta\left(\sum_{j \notin \mathcal{J}(\tilde{\mathbf{x}})}\left(\mathbf{x}_{j}-\mathbf{z}_{j}\right)^{2}\right)$或

  $\begin{aligned}
  L_{\mathrm{H}, \alpha}(\mathbf{x}, \mathbf{z})=& \alpha\left(-\sum_{j \in \mathcal{I}(\tilde{\mathbf{x}})}\left[\mathbf{x}_{j} \log \mathbf{z}_{j}+\left(1-\mathbf{x}_{j}\right) \log \left(1-\mathbf{z}_{j}\right)\right]\right) \\
  &+\beta\left(-\sum_{j \notin \mathcal{J}(\tilde{\mathbf{x}})}\left[\mathbf{x}_{j} \log \mathbf{z}_{j}+\left(1-\mathbf{x}_{j}\right) \log \left(1-\mathbf{z}_{j}\right)\right]\right)
  \end{aligned}$

- 堆叠去噪自编码器：可对nn隐藏层进行逐层加噪去噪。

  ![](fig\fig7\fig2.png)

#### 2.2 图像修复

- 实现方法：屏蔽掉一部分输入，对输入进行预测。

![](fig\fig7\fig4.png)

- 损失函数：L2损失+对抗损失

  $\mathcal{L}_{r e c}(x)=\|\hat{M} \odot(x-F((1-\hat{M}) \odot x))\|_{2}^{2}$

  $\begin{array}{rl}
  \mathcal{L}_{a d v}=\max _{D} & \mathbb{E}_{x \in \mathcal{X}}[\log (D(x)) \\
  & +\log (1-D(F((1-\hat{M}) \odot x))]]
  \end{array}$

- 使用不同损失函数补全结果

  ![](fig\fig7\fig5.png)

- 使用不同预训练方法在各种下游任务中的表现

![](fig\fig7\fig6.png)

#### 2.3 图像上色

- 根据输入灰度图像，预测图片上的颜色信息。

  ![](fig\fig7\fig7.png)

- 上色结果

  ![](fig\fig7\fig8.png)

#### 2.4 Split-Brain 自编码器

- 这种自编码器模型将输入通道分离，然后分别预测彼此，再连接、重构输入；
- ![](fig\fig7\fig9.png)

- 举例：上面分支由RGB预测深度，下面分支有深度预测RGB。

  ![](fig\fig7\fig10.png)



#### 2.5 预测图像上部分内容的相对位置

![](fig\fig7\fig11.png)

#### 2.6 检测图片旋转角度

- 可将原始图片旋转任意角度，然后预测旋转角度。

  - 可应用的方面：根据图片推测相机拍照角度等参数。

- 网络结构

  ![](fig\fig7\fig12.png)

#### 2.7 词向量 word2vec

- 在自然语言处理(NLP)中，需要对语言进行数字化编码，one-hot型编码显然不合适；
- 词向量(word embeddings)就是研究字符到向量编码，有两种基本的编码学习方式：
  - 1) 由输入的上下文推测输入内容(Continuous Bag Of Words)
  - 2) 由输入推测其上下的内容(Skip Gram)

![](fig\fig7\fig13.png)

- 良好的编码结果应该使得具有相似语义的词语在向量空间上也接近，并且相似词语间对照关系的向量也应该近似平行，例如，*国家--首都*

  ![](fig\fig7\fig14.png)

![](fig\fig7\fig15.png)

#### 2.8 对比预测编码

##### 2.8.1 CPC模型结构与损失函数

- Contrastive Predictive Coding，CPC执行任务为：在隐空间中，根据过去时刻信号、预测未来时刻信号(即下图中由$c$预测$z$)。 

![](fig\fig7\fig16.png)

- 优化目标函数为隐空间中过去值与未来值之间的互信息:

  $I(x ; c)=\sum_{x, c} p(x, c) \log \frac{p(x \mid c)}{p(x)}$

- 网络结构如下图：
  - 序列数据的每一帧都经过相同的编码器$g_{enc}$转化为隐变量$z$；
  - 使用自回归模型$g_{ar}$对过去值进行建模，生成$t$时刻的$c_t$；
  - 根据$c_t$与未来值对应的隐变量计算损失函数，一些相关的公式如下：
    - $f_{k}\left(x_{t+k}, c_{t}\right)=\exp \left(z_{t+k}^{T} W_{k} c_{t}\right)$
    - $W_{k} c_{t}$作为预测未来值的隐变量，与真实未来值的隐变量$z_{t+k}^{T}$计算內积，作为相似度的度量；
    - infoNCE: $\mathcal{L}_{\mathrm{N}}=-\underset{X}{\mathbb{E}}\left[\log \frac{f_{k}\left(x_{t+k}, c_{t}\right)}{\sum_{x_{j} \in X} f_{k}\left(x_{j}, c_{t}\right)}\right]$
    - 使用以上损失函数对互信息进行替代；

![](fig\fig7\fig17.png)

##### 2.8.2 CPC特征表示

- 经过上图网络结构对原始数据的抽取，特征$z$相较于原始数据$x$来说时间分辨率下降了(采样频率)，得到了一些slow feature，比如语音信号中的语调与发音，忽略了一些噪声与细节，如下图：

![](fig\fig7\fig18.png)

- 将不同说话人的语音输入模型，根据隐藏层$z$可区分不同说话人说的语音(类似自动产生了聚类效果)，降维可视化如下：

<img src="fig\fig7\fig19.png" style="zoom:50%;" />

- 在语音分类任务中，由不同特征产生分类准确率对比，CPC特征已经接近监督式学习性能。

<img src="fig\fig7\fig20.png" style="zoom:50%;" />

##### 2.8.3  2D CPC

- CPC模型也可扩展至对于二维图像的建模，即将图像分成一系列的patch，根据前几个patch，结合自回归模型，预测后几个patch的隐藏特征。

<img src="fig\fig7\fig21.png" style="zoom:50%;" />

- 一些无监督模型在ImageNet上分类的准确率对比：

  <img src="fig\fig7\fig22.png" style="zoom:50%;" />

- CPC v2 采用大规模的神经网络、复杂数据增强、超长的训练时间等，在自监督特征提取方面、不同的视觉任务上相较CPC v1都有了提升，具体可参看原论文。

  <img src="fig\fig7\fig23.png" style="zoom:50%;" />

- CPC在自然语言处理与强化学习上也有一些应用，具体可学习原课程。

  

#### 2.9 实例辨析

- 实例辨析的任务为，判别两个输入样本是同一物体的不同形态，还是输入是两个完全不同的实体。例如下图所展示的例子。

- 本课程此处介绍了两个对比学习的论文：来自何凯明组的MoCo和来自Hinton组的SimCLR

![](fig\fig7\fig291.png)

##### 2.9.1 Momentum Contrast, MoCo

- MoCo基本框架如下图，

  - 一个输入样本作为查询值(query)，其他待匹配样本作为键(key);
  - 将key, query输入编码其后，对比特征的相似度，并由相似度构对比损失函数；
  - 右边的编码器参数为左边编码器参数的历史加权值(类似带动量的梯度下降SGDM)。

  ![](fig\fig7\fig292.png)

- MoCo使用带温度的Softmax作损失函数：$k_+$代表正例样本(对)

  $\begin{equation}\mathcal{L}_{q}=-\log \frac{\exp \left(q \cdot k_{+} / \tau\right)}{\sum_{i=0}^{K} \exp \left(q \cdot k_{i} / \tau\right)}\end{equation}$

- MoCo伪代码如下：

  ![](fig\fig7\fig293.png)

- 关于为什么设计动量解码器
  - 传统的对比学习(实例辨析)算法，将正例与负例同时采样生成一个batch (下图a),这种原始的做法通常需要一个很大的batch size。
  - 另外一种方式是将样本的表示存储为一个bank (下图b)，每次从中取出样本，直接送入损失函数。但这种方法在一轮训练后，需要用encoder对所有样本进行一次编码，比较费时。
  - MoCo则引入动态调整样本、网络参数，课程中还讲到这样可避免网络与最近的负例太过相关。
  - ![](fig\fig7\fig294.png)
  - 三种方法在不同batch size时的准确率对比：

![](fig\fig7\fig295.png)

- 不同参数量的自监督模型接线性分类器时的准确率：

  ![](fig\fig7\fig296.png)

##### 2.9.2 SimCLR

- SimCLR则偏向于原始端到端的设计，在模型中添加了对数据增强函数的学习，并且在数据表征$\mathbf{h}$之后再通过一个nn $g(\cdot)$。

![](fig\fig7\fig297.png)

- SimCLR伪代码如下：

<img src="fig\fig7\fig298.png" style="zoom:50%;" />

- 注意：SimCLR 的batch size达到了上千。

- 不同参数量的自监督模型接线性分类器时的准确率：
  - SimCLR在同等参数量下，在自监督模型中达到最高的Top-1准确率；
  - SimCLR 在四倍宽度nn条件下，准确率达到监督式学习的水平。

![](fig\fig7\fig299_1.png)

- 在SimCLR提出后，MoCo作者借鉴了SimCLR中的数据增强方式、顶端的MLP结构，提出了MoCo v2。以下是一些对比消融实验结果。

![](fig\fig7\fig301.png)

![](fig\fig7\fig302.png)

#### 3 结论

- 自监督学习算是ML中比较新的一个方向，因其能够从数据中自动产生标签，进行监督式的训练，可从大量无人工标注的数据中学习数据的表征，而获得了大量的关注；
- 窃以为自监督学习在标签生成、模型结构设计方面仍有很大发展空间。











